{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate Images\n",
    "\n",
    "This section of the code through directories of .png .jpg .jpeg or .webp files and edits/pads them in order to align eyes to save to .png with padding\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "To run this code, you need two models in your working directory:\n",
    "- `shape_predictor_5_face_landmarks.dat`: This model enables the face detection capability.\n",
    "- `FSRCNN_x2.pb`: This model enables the 2x upscaling of detected faces, allowing for faster processing and good quality images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing single image\n",
      "filtering images\n",
      "converting to png\n",
      "cropping face\n",
      "finding eyes\n",
      "centering eyes\n",
      "upscalling from fsrcnn\n",
      "processing single image\n",
      "filtering images\n",
      "converting to png\n",
      "cropping face\n",
      "finding eyes\n",
      "centering eyes\n",
      "upscalling from fsrcnn\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_5_face_landmarks.dat')\n",
    "\n",
    "def filter_images(image):\n",
    "    print('filtering images')\n",
    "    if image is not None and image.shape[0] > 211 and image.shape[1] > 211:\n",
    "        faces = detector(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), upsample_num_times=0)\n",
    "        return faces if len(faces) > 0 else None\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def find_eyes(image, face):\n",
    "    print('finding eyes')\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    shape = predictor(gray, face)\n",
    "    features = []\n",
    "    for i in range(0, 5):\n",
    "        features.append((i, (shape.part(i).x, shape.part(i).y)))\n",
    "    return (int(features[3][1][0] + features[2][1][0]) // 2, int(features[3][1][1] + features[2][1][1]) // 2), \\\n",
    "           (int(features[1][1][0] + features[0][1][0]) // 2, int(features[1][1][1] + features[0][1][1]) // 2)\n",
    "\n",
    "def upscale_image_fsrcnn(image, model_path):\n",
    "    print('upscalling from fsrcnn')\n",
    "    # Load the pre-trained FSRCNN_x2 model\n",
    "    sr = cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "    sr.readModel(model_path)\n",
    "    # Set the model name and scale factor\n",
    "    sr.setModel(\"fsrcnn\", 2)\n",
    "    # Upscale the image\n",
    "    upscaled_image = sr.upsample(image)\n",
    "    return upscaled_image\n",
    "\n",
    "def center_eyes(img, left_eye, right_eye, model_path=\"FSRCNN_x2.pb\"):\n",
    "    print('centering eyes')\n",
    "    center = ((left_eye[0] + right_eye[0]) // 2, (left_eye[1] + right_eye[1]) // 2)\n",
    "    dy = right_eye[1] - left_eye[1]\n",
    "    dx = right_eye[0] - left_eye[0]\n",
    "    angle = math.degrees(math.atan2(dy, dx))\n",
    "    height, width, _ = img.shape\n",
    "    eye_distance = int(math.sqrt((right_eye[0] - left_eye[0]) ** 2 + (right_eye[1] - left_eye[1]) ** 2))\n",
    "\n",
    "    # Add border to the original image with increased blur\n",
    "    padding = 512\n",
    "    blur_radius = 125\n",
    "    blur_radius2 = 85\n",
    "    blurred_img = cv2.GaussianBlur(img, (blur_radius, blur_radius2), 0)\n",
    "    padded_blurred_img = cv2.copyMakeBorder(blurred_img, padding, padding, padding, padding, cv2.BORDER_REFLECT_101)\n",
    "    padded_img = cv2.copyMakeBorder(img, padding, padding, padding, padding, cv2.BORDER_REFLECT_101)\n",
    "\n",
    "    alpha = np.zeros((height + 2 * padding, width + 2 * padding, 1), dtype=np.float32)\n",
    "    alpha[padding:height + padding, padding:width + padding] = 1\n",
    "    alpha_3channels = np.repeat(alpha, 3, axis=-1)  # Broadcast alpha to 3 channels\n",
    "    alpha_3channels = alpha_3channels.astype(np.float64)  # Convert alpha to the same type as input images\n",
    "    \n",
    "    # Combine the original image and the blurred image using the alpha channel\n",
    "    blended_img = cv2.multiply(padded_img.astype(np.float64), alpha_3channels) + cv2.multiply(padded_blurred_img.astype(np.float64), 1 - alpha_3channels)\n",
    "    blended_img = blended_img.astype(np.uint8)\n",
    "\n",
    "    # Update the center point after adding the border\n",
    "    center = (center[0] + padding, center[1] + padding)\n",
    "\n",
    "    M_rotate = cv2.getRotationMatrix2D(center, angle, 1)\n",
    "    rotated_img = cv2.warpAffine(blended_img, M_rotate, (width + 2 * padding, height + 2 * padding), flags=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    rotated_center = tuple(map(int, M_rotate.dot(np.array([center[0], center[1], 1]))))\n",
    "\n",
    "    dx_translation = (width + 2 * padding) // 2 - rotated_center[0]\n",
    "    dy_translation = (height + 2 * padding) // 2 - rotated_center[1]\n",
    "    M_translate = np.float32([[1, 0, dx_translation], [0, 1, dy_translation]])\n",
    "    translated_img = cv2.warpAffine(rotated_img, M_translate, (width + 2 * padding, height + 2 * padding), flags=cv2.INTER_LANCZOS4)\n",
    "\n",
    "    reference_eye_distance = 260\n",
    "    scaling_factor = reference_eye_distance / eye_distance\n",
    "    #print(\"eye distance\",eye_distance)\n",
    "    # Resize the image based on the scaling factor\n",
    "    padded_height, padded_width, _ = translated_img.shape\n",
    "    scaled_height = int(padded_height * scaling_factor)\n",
    "    scaled_width = int(padded_width * scaling_factor)\n",
    "\n",
    "    if scaling_factor > 1:\n",
    "        try:\n",
    "            translated_img = upscale_image_fsrcnn(translated_img, model_path)\n",
    "        except cv2.error as e:\n",
    "            print(f\"Error while upscaling image: {e}\")\n",
    "            return None  # return None if upscaling fails\n",
    "    if translated_img is not None:\n",
    "        scaled_img = cv2.resize(translated_img, (scaled_width, scaled_height), interpolation=cv2.INTER_LANCZOS4)\n",
    "        # Extract the central 1024x1024 block\n",
    "        center_y = scaled_height // 2\n",
    "        center_x = scaled_width // 2\n",
    "        cropped_img = scaled_img[center_y - 512:center_y + 512, center_x - 512:center_x + 512]\n",
    "        return cropped_img\n",
    "    return None  # return None if translated_img is None\n",
    "\n",
    "def crop_face(image, face):\n",
    "    print('cropping face')\n",
    "    left_eye, right_eye = find_eyes(image, face)\n",
    "    return center_eyes(image, left_eye, right_eye)\n",
    "\n",
    "def convert_to_png(image, faces, output_path):\n",
    "    print('converting to png')\n",
    "    for face in faces:\n",
    "        cropped_face = crop_face(image, face)\n",
    "        if cropped_face is None or cropped_face.shape[0] < 212 or cropped_face.shape[1] < 212:\n",
    "            continue  # skip this face if cropping fails or the cropped image is too small\n",
    "        face_area = (face.bottom() - face.top()) * (face.right() - face.left())\n",
    "        random_suffix = ''.join(random.choices(string.ascii_letters, k=4))\n",
    "        file_name = f'{face_area}_{random_suffix}.png'\n",
    "        output_image_path = os.path.join(output_path, file_name)\n",
    "        cv2.imwrite(output_image_path, cropped_face)\n",
    "\n",
    "\n",
    "def process_single_image(input_image_path, output_dir):\n",
    "    print('processing single image')\n",
    "    image = cv2.imread(input_image_path)\n",
    "    faces = filter_images(image)\n",
    "    if faces is not None:\n",
    "        relative_path = os.path.relpath(os.path.dirname(input_image_path), input_dir)\n",
    "        output_subdir = os.path.join(output_dir, relative_path)\n",
    "        os.makedirs(output_subdir, exist_ok=True)\n",
    "        # Check if at least one face is greater than or equal to 512x512.\n",
    "        if any((face.right() - face.left()) * (face.bottom() - face.top()) >= 212 * 212 for face in faces):\n",
    "            convert_to_png(image, faces, output_subdir)\n",
    "\n",
    "\n",
    "\n",
    "def process_images(input_dir, output_dir):\n",
    "    valid_extensions = {'.png', '.jpg', '.jpeg', '.webp'}\n",
    "    image_paths = [os.path.join(root, file) for root, _, files in os.walk(input_dir) for file in files\n",
    "                   if os.path.splitext(file.lower())[1] in valid_extensions and\n",
    "                   not any(x in file.lower() for x in [\"instagram\", \"scan\", \"magazine\"])]\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        process_single_image(image_path, output_dir)\n",
    "\n",
    "\n",
    "input_dir = 'images_uncropped'\n",
    "output_dir = 'faces-cropped'\n",
    "process_images(input_dir, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
